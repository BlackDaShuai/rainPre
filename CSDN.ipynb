{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:03.258894Z",
     "start_time": "2024-10-12T15:38:03.242294Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:03.290133Z",
     "start_time": "2024-10-12T15:38:03.277135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path = 'overDataSet'\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 16\n",
    "SEQ_SIZE = 17\n",
    "save_interval = 5"
   ],
   "id": "a4cec4a0f80a4d78",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:03.321283Z",
     "start_time": "2024-10-12T15:38:03.309283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集类\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, dir_path, seq_size=17, transform=None):\n",
    "        self.img_paths = sorted(os.listdir(dir_path), key=lambda x: int(x[:-4]))  # 按数字顺序排序\n",
    "        self.seq_size = seq_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_seq = []\n",
    "        for i in range(self.seq_size):\n",
    "            img_path = os.path.join(dir_path, self.img_paths[index + i])\n",
    "            img = Image.open(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            img_seq.append(img)\n",
    "        label_path = os.path.join(dir_path, self.img_paths[index + self.seq_size])\n",
    "        label = Image.open(label_path)\n",
    "        if self.transform:\n",
    "            label = self.transform(label)\n",
    "        return torch.stack(img_seq), label"
   ],
   "id": "3e4e1493af526890",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:03.352844Z",
     "start_time": "2024-10-12T15:38:03.339847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 图像变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # 调整大小\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_data = SeqDataset(dir_path=dir_path, seq_size=SEQ_SIZE, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ],
   "id": "73320a281ec1f308",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:03.399962Z",
     "start_time": "2024-10-12T15:38:03.371329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 编码器\n",
    "class EncoderMUG2d_LSTM(nn.Module):\n",
    "    def __init__(self, input_nc=3, encode_dim=1024, lstm_hidden_size=1024, seq_len=SEQ_SIZE, num_lstm_layers=1, bidirectional=False):\n",
    "        super(EncoderMUG2d_LSTM, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 32, 4, 2, 1),  # 32*64*64\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 64*32*32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 128*16*16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 256*8*8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 512*4*4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),  # 512*2*2 \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1024, 4, 2, 1),  # 1024*1*1\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1024, encode_dim)\n",
    "        self.lstm = nn.LSTM(encode_dim, lstm_hidden_size, num_layers=num_lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        print(f\"B (batch_size): {B}\")\n",
    "        print(f\"x shape before reshape: {x.shape}\")\n",
    "        x = x.view(B * SEQ_SIZE, 3, 128, 128)  # [batchsize*seqsize, 3, 128, 128]\n",
    "        print(f\"x shape after reshape: {x.shape}\")\n",
    "        x = self.encoder(x)  # [batchsize*seqsize, 1024, 1, 1]\n",
    "        print(f\"x shape after encoder: {x.shape}\")\n",
    "        x = x.view(-1, 1024)  # [batchsize * seqsize, 1024]\n",
    "        print(f\"x shape after view: {x.shape}\")\n",
    "        x = self.fc(x)  # [batchsize * seqsize, encode_dim]\n",
    "        print(f\"x shape after fc: {x.shape}\")\n",
    "        x = x.view(B, SEQ_SIZE, x.size(1))  # [batchsize , seqsize , encode_dim][16,17,1024]\n",
    "        print(f\"x shape after final view: {x.shape}\")\n",
    "        \n",
    "        h0 = torch.zeros(self.num_directions * self.num_lstm_layers, B, self.lstm_hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_directions * self.num_lstm_layers, B, self.lstm_hidden_size).to(x.device)\n",
    "        print(f\"h0 shape: {h0.shape}\")\n",
    "        print(f\"c0 shape: {c0.shape}\")\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return hn[-1]  # 取最后一层的隐藏状态\n",
    "\n",
    "# 解码器\n",
    "class DecoderMUG2d(nn.Module):\n",
    "    def __init__(self, output_nc=3, encode_dim=1024):\n",
    "        super(DecoderMUG2d, self).__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(encode_dim, 1024 * 1 * 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4),  # 512*4*4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2),  # 256*10*10\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4),  # 128*13*13\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2),  # 64*28*28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, output_nc, 4, stride=2, padding=1),  # 3*128*128\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project(x)\n",
    "        x = x.view(-1, 1024, 1, 1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "# 综合模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.encoder = EncoderMUG2d_LSTM()\n",
    "        self.decoder = DecoderMUG2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return self.decoder(encoded)"
   ],
   "id": "283b9732dcc93bc2",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:38:05.583962Z",
     "start_time": "2024-10-12T15:38:03.417059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练过程\n",
    "def train_model():\n",
    "    model = Net()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            inputs, labels = batch_x.cuda(), batch_y.cuda()\n",
    "            #print(f\"inputs shape: {inputs.shape}\")  # 确保输入形状正确\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Loss: {train_loss / len(train_loader):.4f}')\n",
    "\n",
    "        # 保存模型\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            torch.save(model.state_dict(), f'model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "\n",
    "\n",
    "train_model()"
   ],
   "id": "b603746632e1020c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "B (batch_size): 16\n",
      "x shape before reshape: torch.Size([16, 17, 3, 128, 128])\n",
      "x shape after reshape: torch.Size([272, 3, 128, 128])\n",
      "x shape after encoder: torch.Size([272, 1024, 1, 1])\n",
      "x shape after view: torch.Size([272, 1024])\n",
      "x shape after fc: torch.Size([272, 1024])\n",
      "x shape after final view: torch.Size([16, 17, 1024])\n",
      "h0 shape: torch.Size([1, 16, 1024])\n",
      "c0 shape: torch.Size([1, 16, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BLKDASH\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([16, 3, 128, 128])) that is different to the input size (torch.Size([16, 3, 56, 56])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (56) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[105], line 33\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m save_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     29\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_epoch_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[105], line 20\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     19\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m---> 20\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\nn\\functional.py:3383\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3381\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3383\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310torch24cuda124\\lib\\site-packages\\torch\\functional.py:77\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (56) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "execution_count": 105
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
