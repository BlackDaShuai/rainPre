{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.204305Z",
     "start_time": "2024-10-13T07:06:29.151952Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from debugpy.launcher import channel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.234748Z",
     "start_time": "2024-10-13T07:06:29.212960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path = 'overDataSet'\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 16\n",
    "SEQ_SIZE = 17\n",
    "save_interval = 5"
   ],
   "id": "a4cec4a0f80a4d78",
   "outputs": [],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.297288Z",
     "start_time": "2024-10-13T07:06:29.269485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据集类\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, dir_path, seq_size=17, transform=None):\n",
    "        self.img_paths = sorted(os.listdir(dir_path), key=lambda x: int(x[:-4]))  # 按数字顺序排序\n",
    "        self.seq_size = seq_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_seq = []\n",
    "        for i in range(self.seq_size):\n",
    "            img_path = os.path.join(dir_path, self.img_paths[index + i])\n",
    "            img = Image.open(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            img_seq.append(img)\n",
    "        label_path = os.path.join(dir_path, self.img_paths[index + self.seq_size])\n",
    "        label = Image.open(label_path)\n",
    "        if self.transform:\n",
    "            label = self.transform(label)\n",
    "        return torch.stack(img_seq), label"
   ],
   "id": "3e4e1493af526890",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.342812Z",
     "start_time": "2024-10-13T07:06:29.314809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 图像变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # 调整大小\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_data = SeqDataset(dir_path=dir_path, seq_size=SEQ_SIZE, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True, shuffle=False)"
   ],
   "id": "73320a281ec1f308",
   "outputs": [],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.373013Z",
     "start_time": "2024-10-13T07:06:29.360827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义损失函数\n",
    "# ssim = SSIM(data_range=1.0, k1=0.01, k2=0.03,  reduction='sum')"
   ],
   "id": "efd43e2b18e33e95",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:06:29.418713Z",
     "start_time": "2024-10-13T07:06:29.391046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderMUG2d_LSTM(nn.Module):\n",
    "    def __init__(self, input_nc=3, encode_dim=1024, lstm_hidden_size=1024, seq_len=SEQ_SIZE, num_lstm_layers=1, bidirectional=False):\n",
    "        super(EncoderMUG2d_LSTM, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        #3*128*128\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 32, 4,2,1), # 32*64*64\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #32*63*63\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # 64*32*32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #64*31*31\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # 128*16*16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    " \n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # 256*8*8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    " \n",
    "            nn.Conv2d(256, 512, 4, 2, 1), # 512*4*4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    " \n",
    "            nn.Conv2d(512, 512, 4, 2, 1),  # 512*2*2 \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    " \n",
    "            nn.Conv2d(512, 1024, 4, 2, 1),  # 1024*1*1\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    " \n",
    "        )\n",
    " \n",
    "        self.fc = nn.Linear(1024, encode_dim)\n",
    "        self.lstm = nn.LSTM(encode_dim, encode_dim, batch_first=True)\n",
    " \n",
    "    def init_hidden(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h = x.data.new(\n",
    "                self.num_directions * self.num_lstm_layers, batch_size, self.lstm_hidden_size).zero_()\n",
    "        c = x.data.new(\n",
    "                self.num_directions * self.num_lstm_layers, batch_size, self.lstm_hidden_size).zero_()\n",
    "        return torch.Tensor(h), torch.Tensor(c)\n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "        #x.shape [batchsize,seqsize,3,128,128]\n",
    "        B = x.size(0)\n",
    "        x = x.view(B * SEQ_SIZE, 3, 128, 128) #x.shape[batchsize*seqsize,3,128,128]\n",
    "        # [batchsize*seqsize, 3, 128, 128] -> [batchsize*seqsize, 1024,1,1]\n",
    "        x = self.encoder(x)\n",
    "        #[batchsize * seqsize, 1024, 1, 1]-> [batchsize*seqsize, 1024]\n",
    "        x = x.view(-1, 1024)\n",
    "        # [batchsize * seqsize, 1024]\n",
    "        x = self.fc(x)\n",
    "        # [batchsize , seqsize ,1024]\n",
    "        x = x.view(-1, SEQ_SIZE, x.size(1))\n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        output, (hn,cn) = self.lstm(x,(h0,c0))\n",
    "        return hn\n",
    " \n",
    "class DecoderMUG2d(nn.Module):\n",
    "    def __init__(self, output_nc=3, encode_dim=1024): #output size: 64x64\n",
    "        super(DecoderMUG2d, self).__init__()\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(encode_dim, 1024*1*1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4), # 512*4*4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2), # 256*10*10\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(256, 128, 4), # 128*13*13\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(128, 64, 4,stride=2),  # 64*28*28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(64, 32, 4),  # 32*31*31\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(32, 16, 4,stride=2),  # 16*64*64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(16, output_nc, 4, stride=2, padding=1),  # 3*128*128\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.project(x)\n",
    "        x = x.view(-1, 1024, 1, 1)\n",
    "        decode = self.decoder(x)\n",
    "        return decode\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.n1 = EncoderMUG2d_LSTM()\n",
    "        self.n2 = DecoderMUG2d()\n",
    " \n",
    "    def forward(self, x):\n",
    "        output = self.n1(x)\n",
    "        output = self.n2(output) #B*3*128*128\n",
    "        return output"
   ],
   "id": "283b9732dcc93bc2",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-13T07:06:29.437060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def weighted_mse_loss(output, target, weight):\n",
    "    return torch.mean(weight * (output - target) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "# 训练过程\n",
    "def train_model():\n",
    "    model = Net()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            inputs, labels = batch_x.cuda(), batch_y.cuda()\n",
    "            #print(f\"inputs shape: {inputs.shape}\")  # 确保输入形状正确\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #loss = loss_func(outputs, labels)\n",
    "            # 动态计算权重，对白色背景区域赋予较小权重\n",
    "            weight = torch.where(labels < 0.95, 1.0, 0.1)  # 对非白色区域赋予更高权重\n",
    "            loss = weighted_mse_loss(outputs, labels, weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Loss: {train_loss / len(train_loader):.4f}')\n",
    "        \n",
    "        if (epoch + 1) % 1 == 0:  # 每 1 次，保存一下解码的图片和原图片\n",
    "            fake_image = outputs.data.cpu().squeeze()\n",
    "            real_image = labels.data.cpu().squeeze()\n",
    "            if not os.path.exists('./conv_autoencoder'):\n",
    "                os.mkdir('./conv_autoencoder')\n",
    "            utils.save_image(fake_image, './conv_autoencoder/decode_image_{}.png'.format(epoch + 1))\n",
    "            utils.save_image(real_image, './conv_autoencoder/raw_image_{}.png'.format(epoch + 1))\n",
    "        if epoch >= 40:\n",
    "            learning_rate = learning_rate * 0.5\n",
    "\n",
    "        # 保存模型\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model_csdn_lstm.pth')\n",
    "\n",
    "\n",
    "\n",
    "train_model()"
   ],
   "id": "b603746632e1020c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Loss: 0.0235\n",
      "Epoch 2/100\n",
      "Loss: 0.0157\n",
      "Epoch 3/100\n",
      "Loss: 0.0113\n",
      "Epoch 4/100\n",
      "Loss: 0.0094\n",
      "Epoch 5/100\n",
      "Loss: 0.0086\n",
      "Epoch 6/100\n",
      "Loss: 0.0083\n",
      "Epoch 7/100\n",
      "Loss: 0.0082\n",
      "Epoch 8/100\n",
      "Loss: 0.0081\n",
      "Epoch 9/100\n",
      "Loss: 0.0080\n",
      "Epoch 10/100\n",
      "Loss: 0.0080\n",
      "Epoch 11/100\n",
      "Loss: 0.0080\n",
      "Epoch 12/100\n",
      "Loss: 0.0079\n",
      "Epoch 13/100\n",
      "Loss: 0.0078\n",
      "Epoch 14/100\n",
      "Loss: 0.0078\n",
      "Epoch 15/100\n",
      "Loss: 0.0077\n",
      "Epoch 16/100\n",
      "Loss: 0.0077\n",
      "Epoch 17/100\n",
      "Loss: 0.0076\n",
      "Epoch 18/100\n",
      "Loss: 0.0075\n",
      "Epoch 19/100\n",
      "Loss: 0.0076\n",
      "Epoch 20/100\n",
      "Loss: 0.0077\n",
      "Epoch 21/100\n",
      "Loss: 0.0077\n",
      "Epoch 22/100\n",
      "Loss: 0.0077\n",
      "Epoch 23/100\n",
      "Loss: 0.0075\n",
      "Epoch 24/100\n",
      "Loss: 0.0073\n",
      "Epoch 25/100\n",
      "Loss: 0.0071\n",
      "Epoch 26/100\n",
      "Loss: 0.0074\n",
      "Epoch 27/100\n",
      "Loss: 0.0071\n",
      "Epoch 28/100\n",
      "Loss: 0.0071\n",
      "Epoch 29/100\n",
      "Loss: 0.0068\n",
      "Epoch 30/100\n",
      "Loss: 0.0067\n",
      "Epoch 31/100\n",
      "Loss: 0.0069\n",
      "Epoch 32/100\n",
      "Loss: 0.0066\n",
      "Epoch 33/100\n",
      "Loss: 0.0066\n",
      "Epoch 34/100\n",
      "Loss: 0.0073\n",
      "Epoch 35/100\n",
      "Loss: 0.0073\n",
      "Epoch 36/100\n",
      "Loss: 0.0066\n",
      "Epoch 37/100\n",
      "Loss: 0.0059\n",
      "Epoch 38/100\n",
      "Loss: 0.0057\n",
      "Epoch 39/100\n",
      "Loss: 0.0056\n",
      "Epoch 40/100\n",
      "Loss: 0.0056\n",
      "Epoch 41/100\n",
      "Loss: 0.0056\n",
      "Epoch 42/100\n",
      "Loss: 0.0065\n",
      "Epoch 43/100\n",
      "Loss: 0.0054\n",
      "Epoch 44/100\n",
      "Loss: 0.0058\n",
      "Epoch 45/100\n",
      "Loss: 0.0054\n",
      "Epoch 46/100\n",
      "Loss: 0.0058\n",
      "Epoch 47/100\n",
      "Loss: 0.0055\n",
      "Epoch 48/100\n",
      "Loss: 0.0052\n",
      "Epoch 49/100\n",
      "Loss: 0.0051\n",
      "Epoch 50/100\n",
      "Loss: 0.0049\n",
      "Epoch 51/100\n",
      "Loss: 0.0049\n",
      "Epoch 52/100\n",
      "Loss: 0.0048\n",
      "Epoch 53/100\n",
      "Loss: 0.0047\n",
      "Epoch 54/100\n",
      "Loss: 0.0046\n",
      "Epoch 55/100\n",
      "Loss: 0.0045\n",
      "Epoch 56/100\n",
      "Loss: 0.0045\n",
      "Epoch 57/100\n",
      "Loss: 0.0044\n",
      "Epoch 58/100\n",
      "Loss: 0.0044\n",
      "Epoch 59/100\n",
      "Loss: 0.0044\n",
      "Epoch 60/100\n",
      "Loss: 0.0044\n",
      "Epoch 61/100\n",
      "Loss: 0.0048\n",
      "Epoch 62/100\n",
      "Loss: 0.0046\n",
      "Epoch 63/100\n",
      "Loss: 0.0048\n",
      "Epoch 64/100\n",
      "Loss: 0.0045\n",
      "Epoch 65/100\n",
      "Loss: 0.0043\n",
      "Epoch 66/100\n",
      "Loss: 0.0042\n",
      "Epoch 67/100\n",
      "Loss: 0.0043\n",
      "Epoch 68/100\n",
      "Loss: 0.0044\n",
      "Epoch 69/100\n",
      "Loss: 0.0044\n",
      "Epoch 70/100\n",
      "Loss: 0.0047\n",
      "Epoch 71/100\n",
      "Loss: 0.0045\n",
      "Epoch 72/100\n",
      "Loss: 0.0052\n",
      "Epoch 73/100\n",
      "Loss: 0.0046\n",
      "Epoch 74/100\n",
      "Loss: 0.0048\n",
      "Epoch 75/100\n",
      "Loss: 0.0043\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[245], line 49\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;66;03m# 保存模型\u001B[39;00m\n\u001B[0;32m     45\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_csdn_lstm.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 49\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[245], line 31\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 31\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  \u001B[38;5;66;03m# 每 1 次，保存一下解码的图片和原图片\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T07:36:38.015350200Z",
     "start_time": "2024-10-13T06:47:54.700913Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a0d224574a441fd2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
